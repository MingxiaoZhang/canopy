# Canopy Web Crawler

A large-scale web crawler built in Python with a modular architecture for extracting HTML, CSS, screenshots, and DOM structures from websites.

## âœ¨ Features

### Core Capabilities
- **Asynchronous crawling** with aiohttp for high performance
- **Modular architecture** with composable features using builder pattern
- **HTML parsing** and intelligent link extraction
- **CSS discovery** and downloading (external + inline)
- **Screenshot capture** with Playwright browser automation
- **DOM tree extraction** with component-level screenshots
- **Graph-based crawling** with intelligent link prioritization

### Crawling Modes
- **Single Domain**: Crawl within starting domains only
- **Cross Domain**: Follow links across domains with limits
- **Whitelist**: Only crawl specified trusted domains
- **Graph Mode**: Intelligent crawling with link prioritization

### Architecture
- **Builder Pattern**: Fluent API for configuring crawler features
- **Feature Composition**: Mix and match screenshots, DOM extraction, graph crawling
- **Organized Storage**: Date-based file structure with domain organization
- **Error Handling**: Comprehensive error handling and rate limiting
- **Docker Support**: Containerized deployment for consistency

## ğŸš€ Quick Start

### Using the New Builder API

```python
from src.crawler import CrawlerBuilder

# Basic crawling
crawler = (CrawlerBuilder(['https://example.com'])
           .max_pages(10)
           .build())

# Advanced crawling with all features
crawler = (CrawlerBuilder(['https://example.com'])
           .max_pages(20)
           .with_screenshots()
           .with_dom_extraction(capture_screenshots=True)
           .with_graph_crawling(
               mode="cross_domain",
               max_depth=3,
               max_domains=5
           )
           .build())

await crawler.crawl()
```

### Docker (Recommended)

```bash
# Build and run with docker-compose
docker-compose up --build

# Or with plain Docker
docker build -t canopy-crawler .
docker run -v $(pwd)/crawl_data:/app/crawl_data canopy-crawler
```

### Local Installation

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install browser dependencies (Linux/Ubuntu)
sudo ./setup_browser.sh
```

## ğŸ“– Usage Examples

### Basic Crawling
```bash
# Run main crawler
python main.py
```

### Graph Crawling Examples
```bash
# Run all graph crawling examples
python examples/graph_crawling.py

# DOM component extraction
python examples/dom_components.py
```

### Builder Pattern Examples

```python
# Screenshots only
crawler = (CrawlerBuilder(['https://httpbin.org/links/3'])
           .max_pages(5)
           .with_screenshots()
           .build())

# DOM extraction with component screenshots
crawler = (CrawlerBuilder(['https://example.com'])
           .max_pages(10)
           .with_dom_extraction(
               max_depth=8,
               capture_screenshots=True
           )
           .build())

# Advanced graph crawling
crawler = (CrawlerBuilder(['https://httpbin.org/links/5'])
           .max_pages(15)
           .with_graph_crawling(
               mode="cross_domain",
               max_depth=2,
               max_domains=3,
               blocked_domains={'spam-domain.com'},
               priority_domains={'example.com'}
           )
           .build())
```

## ğŸ—ï¸ Architecture

### Modular Directory Structure

```
src/
â”œâ”€â”€ crawler/                    # Core crawler logic
â”‚   â”œâ”€â”€ base.py                # BaseCrawler - core functionality
â”‚   â”œâ”€â”€ builder.py             # CrawlerBuilder - fluent API
â”‚   â”œâ”€â”€ result.py              # CrawlResult - data structures
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ features/                   # Composable feature modules
â”‚   â”œâ”€â”€ base.py                # Base feature interface
â”‚   â”œâ”€â”€ screenshot_feature.py  # Screenshot capture
â”‚   â”œâ”€â”€ dom_extraction_feature.py  # DOM tree extraction
â”‚   â”œâ”€â”€ graph_crawling_feature.py  # Graph-based crawling
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/                      # Utility functions
â”‚   â”œâ”€â”€ parser.py              # HTML parsing utilities
â”‚   â”œâ”€â”€ graph_crawler.py       # Graph algorithms
â”‚   â”œâ”€â”€ dom_tree.py            # DOM tree utilities
â”‚   â”œâ”€â”€ rate_limiter.py        # Rate limiting
â”‚   â”œâ”€â”€ error_handler.py       # Error handling
â”‚   â”œâ”€â”€ deduplication.py       # Duplicate detection
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ storage/                    # Storage and persistence
â”‚   â”œâ”€â”€ storage.py             # File storage management
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ monitoring/                 # Monitoring and observability
â”‚   â”œâ”€â”€ monitoring.py          # Crawler monitoring
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ crawler.py                 # Main entry point
```

### Feature Composition

Features are composable modules that can be mixed and matched:

```python
# Any combination of features
crawler = (CrawlerBuilder(urls)
           .with_screenshots(headless=True)           # Optional
           .with_dom_extraction(max_depth=8)          # Optional
           .with_graph_crawling(mode="whitelist")     # Optional
           .build())
```

Each feature has standardized lifecycle hooks:
- `initialize()` - Setup feature
- `before_crawl()` - Pre-crawling setup
- `process_url()` - Process each URL
- `finalize()` - Cleanup resources

## ğŸ“ Output Structure

Crawled data is organized by date and domain:

```
crawl_data/
â”œâ”€â”€ html/2025/09/27/
â”‚   â””â”€â”€ example.com_[hash].html         # Full HTML content
â”œâ”€â”€ css/2025/09/27/
â”‚   â””â”€â”€ example.com_[hash].css          # Extracted CSS
â”œâ”€â”€ screenshot/2025/09/27/
â”‚   â””â”€â”€ example.com_[hash].png          # Full page screenshots
â”œâ”€â”€ component_screenshots/2025/09/27/
â”‚   â”œâ”€â”€ example.com_[hash]_header.png   # Component screenshots
â”‚   â”œâ”€â”€ example.com_[hash]_nav.png
â”‚   â””â”€â”€ example.com_[hash]_main.png
â””â”€â”€ dom_trees/2025/09/27/
    â””â”€â”€ example.com_[hash]_dom_tree.json # DOM structure
```

## âš™ï¸ Configuration

### Builder Pattern Configuration

```python
# Configure individual features
crawler = (CrawlerBuilder(['https://example.com'])
           .max_pages(50)
           .with_screenshots(
               headless=True,
               viewport_width=1920,
               viewport_height=1080
           )
           .with_dom_extraction(
               max_depth=10,
               capture_screenshots=True
           )
           .with_graph_crawling(
               mode="cross_domain",
               max_depth=3,
               max_domains=10,
               allowed_domains={'example.com', 'trusted.com'},
               blocked_domains={'spam.com'},
               priority_domains={'example.com'}
           )
           .build())
```

### Graph Crawling Modes

1. **Single Domain**: `mode="single_domain"`
   - Only crawl within starting domains
   - Safest option for focused crawling

2. **Cross Domain**: `mode="cross_domain"`
   - Follow links across domains with limits
   - Intelligent domain scoring and prioritization

3. **Whitelist**: `mode="whitelist"`
   - Only crawl specified trusted domains
   - Requires `allowed_domains` parameter

## ğŸ³ Docker Benefits

- âœ… No browser dependency issues
- âœ… Consistent environment across platforms
- âœ… Easy deployment anywhere
- âœ… Resource management and isolation
- âœ… Ready for horizontal scaling

## ğŸ”„ Migration from Old Architecture

If you have code using the old `BasicCrawler`, here's how to migrate:

```python
# Old way
crawler = BasicCrawler(urls, max_pages=10, enable_screenshots=True)
await crawler.crawl()

# New way
crawler = (CrawlerBuilder(urls)
           .max_pages(10)
           .with_screenshots()
           .build())
await crawler.crawl()
```

## ğŸ› ï¸ Development

### Local Development
```bash
# Activate environment
source venv/bin/activate

# Run crawler
python main.py

# Run specific examples
python examples/graph_crawling.py
python examples/dom_components.py
```

### Docker Development
```bash
# Development with live reload
docker-compose up --build

# View logs
docker-compose logs -f

# Run specific example in container
docker run canopy-crawler python examples/graph_crawling.py
```

### Testing
```bash
# Run tests
python -m pytest tests/

# Test specific feature
python tests/test_graph_crawling.py
```

## ğŸ—ºï¸ Roadmap

### Completed âœ…
- [x] Modular architecture with builder pattern
- [x] Feature composition system
- [x] Graph-based crawling with link prioritization
- [x] DOM tree extraction with component screenshots
- [x] Organized directory structure
- [x] Comprehensive error handling and rate limiting

### Planned ğŸš§
- [ ] Redis-based distributed queue system
- [ ] Web interface for monitoring and control
- [ ] Database integration for metadata storage
- [ ] REST API for programmatic control
- [ ] Multiple viewport screenshot capture
- [ ] Content analysis and classification
- [ ] Kubernetes deployment configs

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes following the modular architecture
4. Add tests for new features
5. Test with Docker (`docker-compose up --build`)
6. Commit changes (`git commit -m 'Add amazing feature'`)
7. Push to branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built with [Playwright](https://playwright.dev/) for browser automation
- Uses [aiohttp](https://aiohttp.readthedocs.io/) for async HTTP requests
- Powered by [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/) for HTML parsing